#!/usr/bin/env python3
"""
é¢„å®éªŒç»“æœåˆ†æç¨‹åº
åˆ†ææ¨¡å‹åœ¨å¯å›ç­”/ä¸å¯å›ç­”é—®é¢˜ä¸Šçš„IDKè¡¨ç°ï¼Œä»¥åŠè¾“å‡ºé•¿åº¦ä¸IDKç‡çš„å…³ç³»
"""

import json
import argparse
import os
from collections import defaultdict
from typing import List, Dict, Tuple
import statistics

# å°è¯•å¯¼å…¥numpyå’Œmatplotlibï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    import matplotlib.pyplot as plt
    import matplotlib
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œå°†è·³è¿‡å›¾è¡¨ç”ŸæˆåŠŸèƒ½")
    print("å¦‚éœ€ç”Ÿæˆå›¾è¡¨ï¼Œè¯·è¿è¡Œ: pip install matplotlib")


def load_results(jsonl_path: str) -> List[Dict]:
    """åŠ è½½JSONLç»“æœæ–‡ä»¶"""
    results = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    return results


def count_tokens(text: str) -> int:
    """
    ç®€å•çš„tokenè®¡æ•°ï¼ˆæŒ‰ç©ºæ ¼åˆ†è¯ï¼‰
    å¯¹äºæ›´ç²¾ç¡®çš„è®¡æ•°ï¼Œå¯ä»¥ä½¿ç”¨tokenizer
    """
    # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¸­æ–‡æŒ‰å­—ç¬¦æ•°
    words = text.split()
    # ç²—ç•¥ä¼°ç®—ï¼šæ¯ä¸ªè¯çº¦1.3ä¸ªtoken
    return int(len(words) * 1.3)


def analyze_reward_by_answerable(results: List[Dict]) -> Dict:
    """æŒ‰answerableåˆ†ç±»ç»Ÿè®¡å„rewardå€¼çš„æ¯”ä¾‹"""
    answerable_true = [r for r in results if r.get('answerable') is True]
    answerable_false = [r for r in results if r.get('answerable') is False]
    
    # answerable=True: ç»Ÿè®¡reward=1, 0, -1
    reward_true_counts = {1: 0, 0: 0, -1: 0}
    for r in answerable_true:
        reward = r.get('reward', 0)
        if reward in reward_true_counts:
            reward_true_counts[reward] += 1
    
    # answerable=False: ç»Ÿè®¡reward=1, -1
    reward_false_counts = {1: 0, -1: 0}
    for r in answerable_false:
        reward = r.get('reward', 0)
        if reward in reward_false_counts:
            reward_false_counts[reward] += 1
    
    total_true = len(answerable_true) if answerable_true else 1
    total_false = len(answerable_false) if answerable_false else 1
    
    return {
        'answerable_true': {
            'total': len(answerable_true),
            'reward_1_count': reward_true_counts[1],
            'reward_1_rate': reward_true_counts[1] / total_true,
            'reward_0_count': reward_true_counts[0],
            'reward_0_rate': reward_true_counts[0] / total_true,
            'reward_-1_count': reward_true_counts[-1],
            'reward_-1_rate': reward_true_counts[-1] / total_true,
        },
        'answerable_false': {
            'total': len(answerable_false),
            'reward_1_count': reward_false_counts[1],
            'reward_1_rate': reward_false_counts[1] / total_false,
            'reward_-1_count': reward_false_counts[-1],
            'reward_-1_rate': reward_false_counts[-1] / total_false,
        }
    }





def plot_pie_charts(reward_stats: Dict, output_dir: str, filename_prefix: str):
    """ç»˜åˆ¶answerable=Trueå’ŒFalseä¸¤ç§æƒ…å†µä¸‹çš„å›ç­”æ­£ç¡®ç‡é¥¼å›¾
    
    Args:
        reward_stats: æŒ‰answerableåˆ†ç±»çš„rewardç»Ÿè®¡
        output_dir: è¾“å‡ºç›®å½•
        filename_prefix: æ–‡ä»¶åå‰ç¼€
    """
    
    if not HAS_MATPLOTLIB:
        print("  è·³è¿‡é¥¼å›¾ç”Ÿæˆï¼ˆmatplotlibæœªå®‰è£…ï¼‰")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # å·¦å›¾: Answerable=True (åº”è¯¥å›ç­”çš„é—®é¢˜)
    ax1 = axes[0]
    stats_true = reward_stats['answerable_true']
    
    if stats_true['total'] > 0:
        labels = ['Correct Answer\n(Reward=1)', 
                  'Said "I don\'t know"\n(Reward=0)', 
                  'Wrong Answer\n(Reward=-1)']
        sizes = [
            stats_true['reward_1_count'],
            stats_true['reward_0_count'],
            stats_true['reward_-1_count']
        ]
        colors = ['#2ecc71', '#f39c12', '#e74c3c']  # ç»¿è‰²ã€æ©™è‰²ã€çº¢è‰²
        explode = (0.05, 0.05, 0.05)  # ç¨å¾®åˆ†ç¦»æ¯ä¸ªæ‰‡åŒº
        
        wedges, texts, autotexts = ax1.pie(
            sizes, 
            explode=explode,
            labels=labels, 
            colors=colors,
            autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*sum(sizes))})',
            startangle=90,
            textprops={'fontsize': 11, 'fontweight': 'bold'},
            shadow=True
        )
        
        # ç¾åŒ–ç™¾åˆ†æ¯”æ–‡å­—
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(12)
            autotext.set_fontweight('bold')
        
        ax1.set_title(
            f'Answerable=True (Should Answer)\nTotal: {stats_true["total"]} samples',
            fontsize=14,
            fontweight='bold',
            pad=20
        )
    else:
        ax1.text(0.5, 0.5, 'No Data', ha='center', va='center', 
                fontsize=20, transform=ax1.transAxes)
        ax1.set_title('Answerable=True (Should Answer)', fontsize=14, fontweight='bold')
    
    # å³å›¾: Answerable=False (ä¸åº”è¯¥å›ç­”çš„é—®é¢˜)
    ax2 = axes[1]
    stats_false = reward_stats['answerable_false']
    
    if stats_false['total'] > 0:
        labels = ['Correct Refusal\n(Said "I don\'t know")\n(Reward=1)', 
                  'Wrong Answer\n(Should not answer)\n(Reward=-1)']
        sizes = [
            stats_false['reward_1_count'],
            stats_false['reward_-1_count']
        ]
        colors = ['#2ecc71', '#e74c3c']  # ç»¿è‰²ã€çº¢è‰²
        explode = (0.05, 0.05)
        
        wedges, texts, autotexts = ax2.pie(
            sizes,
            explode=explode,
            labels=labels,
            colors=colors,
            autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*sum(sizes))})',
            startangle=90,
            textprops={'fontsize': 11, 'fontweight': 'bold'},
            shadow=True
        )
        
        # ç¾åŒ–ç™¾åˆ†æ¯”æ–‡å­—
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(12)
            autotext.set_fontweight('bold')
        
        ax2.set_title(
            f'Answerable=False (Should Not Answer)\nTotal: {stats_false["total"]} samples',
            fontsize=14,
            fontweight='bold',
            pad=20
        )
    else:
        ax2.text(0.5, 0.5, 'No Data', ha='center', va='center', 
                fontsize=20, transform=ax2.transAxes)
        ax2.set_title('Answerable=False (Should Not Answer)', fontsize=14, fontweight='bold')
    
    plt.suptitle('Model Performance Analysis by Answerability', 
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    
    # ä¿å­˜å›¾ç‰‡
    output_path = os.path.join(output_dir, f'{filename_prefix}_pie_chart.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  é¥¼å›¾å·²ä¿å­˜: {output_path}")
    plt.close()





def print_summary(results: List[Dict], reward_stats: Dict):
    """æ‰“å°åˆ†ææ‘˜è¦"""
    print("\n" + "=" * 80)
    print("é¢„å®éªŒç»“æœåˆ†ææŠ¥å‘Š - Rewardåˆ†å¸ƒåˆ†æ")
    print("=" * 80)
    
    # è®¡ç®—æ€»åˆ†ï¼ˆæ‰€æœ‰æ ·æœ¬rewardçš„å¹³å‡å€¼ï¼‰
    all_rewards = [r.get('reward', 0) for r in results]
    total_score = sum(all_rewards) / len(all_rewards) if all_rewards else 0
    
    print(f"\nã€1. åŸºæœ¬ç»Ÿè®¡ã€‘")
    print(f"  æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"  æ€»åˆ† (å¹³å‡Reward): {total_score:.4f}")
    
    print(f"\nã€2. æŒ‰Answerableåˆ†ç±»çš„Rewardåˆ†å¸ƒã€‘")
    print(f"\n  Answerable=True (åº”è¯¥å›ç­”çš„é—®é¢˜):")
    stats_true = reward_stats['answerable_true']
    print(f"    æ€»æ ·æœ¬æ•°:        {stats_true['total']}")
    print(f"    Reward=1:        {stats_true['reward_1_count']:4d} ({stats_true['reward_1_rate']*100:5.2f}%) - æ­£ç¡®å›ç­”")
    print(f"    Reward=0:        {stats_true['reward_0_count']:4d} ({stats_true['reward_0_rate']*100:5.2f}%) - ä¸åº”æ‹’ç­”ä½†æ‹’ç­”")
    print(f"    Reward=-1:       {stats_true['reward_-1_count']:4d} ({stats_true['reward_-1_rate']*100:5.2f}%) - é”™è¯¯å›ç­”")
    
    print(f"\n  Answerable=False (ä¸åº”è¯¥å›ç­”çš„é—®é¢˜):")
    stats_false = reward_stats['answerable_false']
    print(f"    æ€»æ ·æœ¬æ•°:        {stats_false['total']}")
    print(f"    Reward=1:        {stats_false['reward_1_count']:4d} ({stats_false['reward_1_rate']*100:5.2f}%) - æ­£ç¡®æ‹’ç­”")
    print(f"    Reward=-1:       {stats_false['reward_-1_count']:4d} ({stats_false['reward_-1_rate']*100:5.2f}%) - ä¸åº”å›ç­”ä½†å›ç­”")
    
    print(f"\nã€3. Tokené•¿åº¦ç»Ÿè®¡ã€‘")
    all_token_counts = [count_tokens(r.get('model_output', '')) for r in results]
    if HAS_NUMPY:
        mean_tokens = np.mean(all_token_counts)
        median_tokens = np.median(all_token_counts)
        min_tokens = np.min(all_token_counts)
        max_tokens = np.max(all_token_counts)
    else:
        mean_tokens = statistics.mean(all_token_counts) if all_token_counts else 0
        median_tokens = statistics.median(all_token_counts) if all_token_counts else 0
        min_tokens = min(all_token_counts) if all_token_counts else 0
        max_tokens = max(all_token_counts) if all_token_counts else 0
    
    print(f"    å¹³å‡è¾“å‡ºtokenæ•°: {mean_tokens:.1f}")
    print(f"    ä¸­ä½æ•°:          {median_tokens:.1f}")
    print(f"    æœ€å°å€¼:          {min_tokens}")
    print(f"    æœ€å¤§å€¼:          {max_tokens}")
    
    print("\n" + "=" * 80)
    
    # åœ¨æŠ¥å‘Šæœ«å°¾å†æ¬¡çªå‡ºæ˜¾ç¤ºæ€»åˆ†
    all_rewards = [r.get('reward', 0) for r in results]
    total_score = sum(all_rewards) / len(all_rewards) if all_rewards else 0
    
    print(f"\n{'ğŸ† æ€»åˆ† (å¹³å‡Reward)':^80s}")
    print(f"{'=' * 80}")
    print(f"{total_score:^80.4f}")
    print(f"{'=' * 80}\n")


def main():
    parser = argparse.ArgumentParser(
        description='åˆ†æé¢„å®éªŒç»“æœï¼šIDKè¡¨ç°åˆ†æ'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: prelininary/inference_results/qwen-7b_inference_results.jsonl)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='prelininary/analysis_results',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: prelininary/analysis_results)'
    )
    parser.add_argument(
        '--prefix',
        type=str,
        default='',
        help='è¾“å‡ºæ–‡ä»¶åå‰ç¼€ (é»˜è®¤: ä»è¾“å…¥æ–‡ä»¶åæå–)'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶åå‰ç¼€
    if not args.prefix:
        # ä»è¾“å…¥æ–‡ä»¶åæå–ï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
        basename = os.path.basename(args.input)
        args.prefix = os.path.splitext(basename)[0]
    
    print(f"\næ­£åœ¨åŠ è½½ä¸»æ•°æ®: {args.input}")
    results = load_results(args.input)
    print(f"  å·²åŠ è½½ {len(results)} ä¸ªæ ·æœ¬")
    
    print(f"\næ­£åœ¨åˆ†æRewardåˆ†å¸ƒ...")
    reward_stats = analyze_reward_by_answerable(results)
    
    print(f"\næ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
    print(f"  ç”Ÿæˆé¥¼å›¾...")
    plot_pie_charts(reward_stats, args.output_dir, args.prefix)
    
    # æ‰“å°æ‘˜è¦
    print_summary(results, reward_stats)
    
    print(f"\nâœ“ åˆ†æå®Œæˆï¼")


if __name__ == '__main__':
    main()