"""
æ ¼å¼æ ¡å‡†æ¨¡å— - å®šæœŸ SFT å¾®è°ƒæ¥ç»´æŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›
ä¸¥æ ¼é¿å…æ•°æ®æ³„æ¼ï¼šä½¿ç”¨è®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®ï¼ˆindex 3001-4000ï¼‰
"""

import os
import torch
import numpy as np
from typing import List, Dict, Optional
import pyarrow.parquet as pq
import random
from dataclasses import dataclass


@dataclass
class FormatAnchorConfig:
    """æ ¼å¼æ ¡å‡†é…ç½®"""
    frequency: int = 50              # æ¯Næ­¥æ ¡å‡†ä¸€æ¬¡
    steps_per_anchor: int = 2        # æ¯æ¬¡æ ¡å‡†çš„æ­¥æ•°
    lr_ratio: float = 0.1            # ç›¸å¯¹äºä¸»è®­ç»ƒçš„å­¦ä¹ ç‡æ¯”ä¾‹
    batch_size: int = 16             # æ ¡å‡†batchå¤§å°
    data_start_idx: int = 3001       # æ•°æ®èµ·å§‹ç´¢å¼•ï¼ˆé¿å…æ³„æ¼ï¼‰
    data_end_idx: int = 4000         # æ•°æ®ç»“æŸç´¢å¼•
    format_check_strict: bool = True  # æ˜¯å¦ä¸¥æ ¼æ£€æŸ¥æ ¼å¼
    verbose: bool = True             # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯


class FormatAnchoringDataset:
    """
    æ ¼å¼æ ¡å‡†æ•°æ®é›†
    ç›´æ¥åŠ è½½é¢„å¤„ç†å¥½çš„SFTæ ¼å¼æ•°æ®ï¼ˆæ¥è‡ª format_anchor_data.pyï¼‰
    """
    
    def __init__(
        self,
        data_file: str,  # é¢„å¤„ç†å¥½çš„æ ¼å¼æ ¡å‡†æ•°æ®æ–‡ä»¶è·¯å¾„
        tokenizer,
        max_samples: Optional[int] = None,
    ):
        """
        Args:
            data_file: é¢„å¤„ç†å¥½çš„æ ¼å¼æ ¡å‡†æ•°æ®parquetæ–‡ä»¶è·¯å¾„
            tokenizer: tokenizer
            max_samples: æœ€å¤šåŠ è½½å¤šå°‘ä¸ªæ ·æœ¬ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨åŠ è½½ï¼‰
        """
        self.tokenizer = tokenizer
        self.samples = []
        
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ åŠ è½½æ ¼å¼æ ¡å‡†æ•°æ®")
        print(f"{'='*80}")
        print(f"  æ–‡ä»¶: {data_file}")
        
        self._load_dataset(data_file, max_samples)
        
        print(f"\nâœ“ æ€»å…±åŠ è½½ {len(self.samples)} ä¸ªæ ¼å¼æ ¡å‡†æ ·æœ¬")
        print(f"{'='*80}\n")
    
    def _load_dataset(self, file_path: str, max_samples: Optional[int]):
        """ä»é¢„å¤„ç†å¥½çš„æ•°æ®æ–‡ä»¶åŠ è½½æ ·æœ¬"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                print(f"  âœ— æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return
            
            # è¯»å– parquet æ–‡ä»¶
            table = pq.read_table(file_path)
            df = table.to_pandas()
            
            total_rows = len(df)
            print(f"  âœ“ æ•°æ®é›†å¤§å°: {total_rows} ä¸ªæ ·æœ¬")
            
            # ç¡®å®šå®é™…åŠ è½½æ•°é‡
            if max_samples is not None and max_samples < total_rows:
                print(f"  â„¹ï¸  åªåŠ è½½å‰ {max_samples} ä¸ªæ ·æœ¬")
                df = df.iloc[:max_samples]
            
            # åŠ è½½æ‰€æœ‰æ ·æœ¬
            loaded_count = 0
            for idx, row in df.iterrows():
                # é¢„å¤„ç†æ•°æ®å·²ç»åŒ…å« prompt å’Œ response
                prompt = row.get('prompt', '')
                response = row.get('response', '')
                
                if not prompt or not response:
                    continue
                
                # éªŒè¯æ ¼å¼
                if self._is_format_valid(response):
                    self.samples.append({
                        'prompt': prompt,
                        'response': response,
                        'dataset': row.get('data_source', 'unknown'),
                        'original_idx': row.get('original_idx', idx),
                        'question': row.get('question', ''),
                        'answer': row.get('answer', ''),
                    })
                    loaded_count += 1
            
            # ç»Ÿè®¡å„æ•°æ®é›†æ ·æœ¬æ•°
            from collections import Counter
            source_counts = Counter(s['dataset'] for s in self.samples)
            for source, count in source_counts.items():
                print(f"    {source}: {count} ä¸ªæ ·æœ¬")
            
            print(f"  âœ“ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæœ‰æ•ˆæ ·æœ¬")
            
        except Exception as e:
            print(f"  âœ— åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _is_format_valid(self, response: str) -> bool:
        """æ£€æŸ¥æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        # å¿…é¡»æœ‰ <answer> å’Œ </answer>
        if '<answer>' not in response.lower() or '</answer>' not in response.lower():
            return False
        
        # å¦‚æœæœ‰ <think>ï¼Œå¿…é¡»æœ‰ </think>
        has_think = '<think>' in response.lower()
        has_think_end = '</think>' in response.lower()
        if has_think != has_think_end:
            return False
        
        return True
    
    def sample_batch(self, batch_size: int) -> List[Dict]:
        """éšæœºé‡‡æ ·ä¸€ä¸ª batch"""
        if len(self.samples) < batch_size:
            # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œå…è®¸é‡å¤é‡‡æ ·
            return random.choices(self.samples, k=batch_size)
        else:
            return random.sample(self.samples, batch_size)
    
    def __len__(self):
        return len(self.samples)


class FormatAnchor:
    """
    æ ¼å¼æ ¡å‡†å™¨
    åœ¨ GRPO è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¿›è¡Œ SFT å¾®è°ƒä»¥ç»´æŒæ ¼å¼èƒ½åŠ›
    """
    
    def __init__(
        self,
        config: FormatAnchorConfig,
        tokenizer,
        data_file: str,
    ):
        """
        Args:
            config: æ ¼å¼æ ¡å‡†é…ç½®
            tokenizer: tokenizer
            data_file: é¢„å¤„ç†å¥½çš„æ ¼å¼æ ¡å‡†æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.config = config
        self.tokenizer = tokenizer
        
        # åŠ è½½æ ¡å‡†æ•°æ®é›†
        self.dataset = FormatAnchoringDataset(
            data_file=data_file,
            tokenizer=tokenizer,
            max_samples=None,  # åŠ è½½æ‰€æœ‰æ ·æœ¬
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_anchors = 0
        self.anchor_history = []
    
    def should_anchor(self, global_step: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ ¡å‡†"""
        if global_step == 0:
            return False
        return global_step % self.config.frequency == 0
    
    def anchor(
        self,
        actor_module,
        optimizer,
        device='cuda'
    ) -> Dict[str, float]:
        """
        æ‰§è¡Œæ ¼å¼æ ¡å‡†
        
        Args:
            actor_module: actor æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            device: è®¾å¤‡
            
        Returns:
            Dict: æ ¡å‡†ç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.dataset) == 0:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„æ ¼å¼æ ¡å‡†æ•°æ®")
            return {'anchor_loss': 0.0, 'samples': 0}
        
        # ä¿å­˜åŸå§‹å­¦ä¹ ç‡
        original_lrs = [pg['lr'] for pg in optimizer.param_groups]
        
        # è®¾ç½®æ ¡å‡†å­¦ä¹ ç‡ï¼ˆæ›´å°ï¼‰
        anchor_lr = original_lrs[0] * self.config.lr_ratio
        for param_group in optimizer.param_groups:
            param_group['lr'] = anchor_lr
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        actor_module.train()
        
        total_loss = 0.0
        num_samples = 0
        
        if self.config.verbose:
            print(f"\n{'â”€'*60}")
            print(f"ğŸ”§ æ ¼å¼æ ¡å‡†ä¸­... (LR: {anchor_lr:.2e})")
        
        for step in range(self.config.steps_per_anchor):
            # é‡‡æ ·ä¸€ä¸ª batch
            batch_samples = self.dataset.sample_batch(self.config.batch_size)
            
            # å‡†å¤‡æ•°æ®
            batch_data = self._prepare_batch(batch_samples, device)
            
            # è®¡ç®— SFT loss
            loss = self._compute_sft_loss(actor_module, batch_data)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
            torch.nn.utils.clip_grad_norm_(actor_module.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            num_samples += len(batch_samples)
            
            if self.config.verbose:
                print(f"  Step {step+1}/{self.config.steps_per_anchor}: loss={loss.item():.4f}")
        
        # æ¢å¤åŸå§‹å­¦ä¹ ç‡
        for param_group, original_lr in zip(optimizer.param_groups, original_lrs):
            param_group['lr'] = original_lr
        
        avg_loss = total_loss / self.config.steps_per_anchor
        
        # è®°å½•ç»Ÿè®¡
        self.total_anchors += 1
        self.anchor_history.append({
            'step': self.total_anchors * self.config.frequency,
            'loss': avg_loss,
            'samples': num_samples
        })
        
        if self.config.verbose:
            print(f"  âœ“ æ ¡å‡†å®Œæˆ: å¹³å‡ loss={avg_loss:.4f}, æ ·æœ¬æ•°={num_samples}")
            print(f"{'â”€'*60}\n")
        
        return {
            'anchor_loss': avg_loss,
            'anchor_samples': num_samples,
            'total_anchors': self.total_anchors
        }
    
    def _prepare_batch(self, batch_samples: List[Dict], device) -> Dict:
        """å‡†å¤‡è®­ç»ƒ batch"""
        prompts = [sample['prompt'] for sample in batch_samples]
        responses = [sample['response'] for sample in batch_samples]
        
        # æ‹¼æ¥ prompt + response
        full_texts = [p + r for p, r in zip(prompts, responses)]
        
        # Tokenize
        encodings = self.tokenizer(
            full_texts,
            padding=True,
            truncation=True,
            max_length=2048,
            return_tensors='pt'
        )
        
        input_ids = encodings['input_ids'].to(device)
        attention_mask = encodings['attention_mask'].to(device)
        
        # åˆ›å»º labelsï¼ˆåªåœ¨ response éƒ¨åˆ†è®¡ç®— lossï¼‰
        labels = input_ids.clone()
        
        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œmask æ‰ prompt éƒ¨åˆ†
        for i, (prompt, response) in enumerate(zip(prompts, responses)):
            prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)['input_ids']
            prompt_length = len(prompt_tokens)
            # Mask prompt éƒ¨åˆ†
            labels[i, :prompt_length] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }
    
    def _compute_sft_loss(self, model, batch_data: Dict) -> torch.Tensor:
        """è®¡ç®— SFT loss"""
        input_ids = batch_data['input_ids']
        labels = batch_data['labels']
        attention_mask = batch_data['attention_mask']
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        # å¦‚æœæ¨¡å‹è¿”å› lossï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(outputs, 'loss') and outputs.loss is not None:
            return outputs.loss
        
        # å¦åˆ™æ‰‹åŠ¨è®¡ç®—
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs
        
        # Shift for next token prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # è®¡ç®— cross entropy loss
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
        loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1)
        )
        
        return loss
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.anchor_history) == 0:
            return {
                'total_anchors': 0,
                'avg_loss': 0.0,
                'latest_loss': 0.0
            }
        
        avg_loss = np.mean([h['loss'] for h in self.anchor_history])
        latest_loss = self.anchor_history[-1]['loss']
        
        return {
            'total_anchors': self.total_anchors,
            'avg_loss': avg_loss,
            'latest_loss': latest_loss,
            'history': self.anchor_history
        }


def integrate_format_anchoring(trainer_instance, config: FormatAnchorConfig, data_file: str):
    """
    å°†æ ¼å¼æ ¡å‡†é›†æˆåˆ° RayPPOTrainer ä¸­
    
    Args:
        trainer_instance: RayPPOTrainer å®ä¾‹
        config: æ ¼å¼æ ¡å‡†é…ç½®
        data_file: é¢„å¤„ç†å¥½çš„æ ¼å¼æ ¡å‡†æ•°æ®æ–‡ä»¶è·¯å¾„
        
    ä½¿ç”¨ç¤ºä¾‹:
        from verl.utils.format_anchoring import integrate_format_anchoring, FormatAnchorConfig
        
        # é…ç½®
        anchor_config = FormatAnchorConfig(
            frequency=50,
            steps_per_anchor=2,
            lr_ratio=0.1,
            batch_size=16,
        )
        
        # é¢„å¤„ç†å¥½çš„æ•°æ®æ–‡ä»¶ï¼ˆä½¿ç”¨ preprocess_format_anchor.sh ç”Ÿæˆï¼‰
        data_file = 'data/format_anchor/deepseek-r1-distill-qwen/format_anchor.parquet'
        
        # é›†æˆ
        trainer = RayPPOTrainer(config)
        integrate_format_anchoring(trainer, anchor_config, data_file)
        
        # æ­£å¸¸è®­ç»ƒï¼Œè‡ªåŠ¨åŒ…å«æ ¼å¼æ ¡å‡†
        trainer.fit()
    """
    # åˆ›å»ºæ ¼å¼æ ¡å‡†å™¨
    format_anchor = FormatAnchor(
        config=config,
        tokenizer=trainer_instance.tokenizer,
        data_file=data_file
    )
    
    # ä¿å­˜åˆ° trainer å®ä¾‹
    trainer_instance.format_anchor = format_anchor
    
    return trainer_instance

