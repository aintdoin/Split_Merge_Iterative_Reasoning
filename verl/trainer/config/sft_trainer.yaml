# 基础版本的sft_trainer.yaml配置文件
data:
  train_batch_size: 64         # 全局训练批次大小
  micro_batch_size_per_gpu: 4  # 每个GPU的微批次大小
  train_files: /path/to/your/train_data.parquet  # 训练数据文件路径
  val_files: /path/to/your/val_data.parquet      # 验证数据文件路径
  prompt_key: prompt            # 数据中prompt字段的名称
  response_key: response        # 数据中response字段的名称
  max_length: 8192            # 最大序列长度 (8192 prompt + 64 response)
  truncation: error             # 序列过长时的处理方式(error/left/right)
  chat_template: null           # 暂不支持自定义chat template
  balance_dp_token: False       # 是否在DP rank之间平衡token数量

model:
  partial_pretrain: /path/to/your/model  # 预训练模型路径
  fsdp_config:
    wrap_policy:
      min_num_params: 0
    cpu_offload: False
    offload_params: False
  trust_remote_code: True       # 是否信任远程代码 (Qwen等模型通常需要)
  enable_gradient_checkpointing: True # 启用梯度检查点以节省显存 (长序列训练推荐)
  lora_rank: 0                  # 设置为正数启用LoRA微调（如32）
  lora_alpha: 16                # LoRA缩放因子
  target_modules: all-linear    # LoRA微调的目标模块
  use_liger: False              # 是否使用Liger kernel

optim:
  lr: 2e-5                      # 学习率
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.1       # 预热步数比例
  clip_grad: 1.0                # 梯度裁剪阈值

trainer:
  default_local_dir: ./output   # 输出目录
  total_epochs: 3               # 训练总轮数
  total_training_steps: null    # 可以设置总步数，优先级高于epochs
  project_name: verl_sft        # 项目名称
  experiment_name: sft_exp      # 实验名称
  logger: ['console']           # 日志记录方式
  seed: 42                      # 随机种子
  default_hdfs_dir: null        # HDFS保存路径

ulysses_sequence_parallel_size: 1 # Ulysses序列并行大小，1表示不启用
use_remove_padding: False         # 是否移除padding
