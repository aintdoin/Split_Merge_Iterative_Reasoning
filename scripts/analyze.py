#!/usr/bin/env python3
"""
è§£æ grpo.log æ–‡ä»¶ï¼Œæå– score, hallucination, correct, miss ç­‰æŒ‡æ ‡å¹¶ç»˜åˆ¶æŠ˜çº¿å›¾
"""

import re
import os
import sys
import matplotlib.pyplot as plt
import matplotlib
from collections import defaultdict

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file_path):
    """è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–å„ä¸ªæŒ‡æ ‡æ•°æ®"""
    data = defaultdict(lambda: defaultdict(list))
    current_step = None
    
    with open(log_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # æ£€æµ‹ Step æ ‡è®°
        step_match = re.search(r'Training Sample @ Step (\d+)', line)
        if step_match:
            current_step = int(step_match.group(1))
            print(f"Found Step {current_step}")
        
        # æ£€æµ‹æŒ‡æ ‡å­—å…¸çš„å¼€å§‹ï¼ˆåŒ…å« 'val/test_' çš„è¡Œï¼‰
        if current_step and "'val/test_" in line and '{' in line:
            # è¯»å–å®Œæ•´çš„å­—å…¸ï¼Œå¯èƒ½è·¨è¶Šå¤šè¡Œ
            dict_lines = [line]
            bracket_count = line.count('{') - line.count('}')
            
            j = i + 1
            while bracket_count > 0 and j < len(lines):
                dict_lines.append(lines[j])
                bracket_count += lines[j].count('{') - lines[j].count('}')
                j += 1
            
            dict_str = ''.join(dict_lines)
            
            # å»é™¤ ANSI é¢œè‰²ä»£ç 
            dict_str = re.sub(r'\[36m.*?\[0m', '', dict_str)
            dict_str = re.sub(r'\(main_task pid=\d+\)', '', dict_str)
            
            # æå–æŒ‡æ ‡
            metrics = {}
            
            # æå– test_score
            score_matches = re.findall(r"'val/test_score/(\w+)':\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", dict_str)
            for dataset, value in score_matches:
                if 'score' not in metrics:
                    metrics['score'] = {}
                metrics['score'][dataset] = float(value)
            
            # æå– test_hallucination
            hall_matches = re.findall(r"'val/test_hallucination/(\w+)':\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", dict_str)
            for dataset, value in hall_matches:
                if 'hallucination' not in metrics:
                    metrics['hallucination'] = {}
                metrics['hallucination'][dataset] = float(value)
            
            # æå– test_n_correct
            correct_matches = re.findall(r"'val/test_n_correct/(\w+)':\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", dict_str)
            for dataset, value in correct_matches:
                if 'correct' not in metrics:
                    metrics['correct'] = {}
                metrics['correct'][dataset] = float(value)
            
            # æå– test_n_miss
            miss_matches = re.findall(r"'val/test_n_miss/(\w+)':\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", dict_str)
            for dataset, value in miss_matches:
                if 'miss' not in metrics:
                    metrics['miss'] = {}
                metrics['miss'][dataset] = float(value)
            
            # æå– test_answer_score
            answer_score_matches = re.findall(r"'val/test_answer_score/(\w+)':\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", dict_str)
            for dataset, value in answer_score_matches:
                if 'answer_score' not in metrics:
                    metrics['answer_score'] = {}
                metrics['answer_score'][dataset] = float(value)
            
            # å­˜å‚¨æ•°æ®
            if metrics:
                for metric_type, datasets in metrics.items():
                    for dataset, value in datasets.items():
                        data[metric_type][dataset].append((current_step, value))
                
            i = j - 1
        
        i += 1
    
    return data


def find_best_step(data):
    """æ‰¾å‡ºä¸‰ä¸ªæ•°æ®é›†å¹³å‡ test_score æœ€é«˜çš„ step"""
    if 'score' not in data:
        return None, None, {}
    
    # è®¡ç®—æ¯ä¸ª step çš„å¹³å‡ score
    step_avg_scores = {}
    step_detailed_scores = {}
    
    # è·å–æ‰€æœ‰ step
    all_steps = set()
    for dataset, values in data['score'].items():
        for step, _ in values:
            all_steps.add(step)
    
    # å¯¹æ¯ä¸ª step è®¡ç®—å¹³å‡åˆ†
    for step in sorted(all_steps):
        scores = []
        details = {}
        for dataset, values in data['score'].items():
            for s, v in values:
                if s == step:
                    scores.append(v)
                    details[dataset] = v
                    break
        
        if scores:
            step_avg_scores[step] = sum(scores) / len(scores)
            step_detailed_scores[step] = details
    
    # æ‰¾åˆ°æœ€é«˜åˆ†çš„ step
    if step_avg_scores:
        best_step = max(step_avg_scores.keys(), key=lambda s: step_avg_scores[s])
        best_avg_score = step_avg_scores[best_step]
        best_details = step_detailed_scores[best_step]
        return best_step, best_avg_score, best_details
    
    return None, None, {}


def plot_combined_metrics(data, output_path):
    """ç»˜åˆ¶ç»¼åˆæŒ‡æ ‡æŠ˜çº¿å›¾ï¼ˆæ‰€æœ‰æŒ‡æ ‡åœ¨ä¸€èµ·ï¼Œæ¯ä¸ªæŒ‡æ ‡ä¸€ä¸ªå­å›¾ï¼‰"""
    
    metric_names = {
        'score': 'Score',
        'hallucination': 'Hallucination',
        'correct': 'Correct',
        'miss': 'Miss'
    }
    
    # åˆ›å»º 2x2 å­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Training Metrics Over Steps', fontsize=16, fontweight='bold')
    
    metrics_to_plot = ['score', 'hallucination', 'correct', 'miss']
    
    for idx, metric_type in enumerate(metrics_to_plot):
        ax = axes[idx // 2, idx % 2]
        
        if metric_type in data:
            for dataset, values in data[metric_type].items():
                if values:
                    steps = [v[0] for v in values]
                    vals = [v[1] for v in values]
                    ax.plot(steps, vals, marker='o', label=dataset, linewidth=2, markersize=5)
        
        ax.set_xlabel('Step', fontsize=11)
        ax.set_ylabel(metric_names[metric_type], fontsize=11)
        ax.set_title(metric_names[metric_type], fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved combined plot: {output_path}")
    plt.close()


if __name__ == '__main__':
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å– log æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„
    log_file = sys.argv[1]
    
    # è·å– log æ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
    log_basename = os.path.basename(log_file)
    log_name = os.path.splitext(log_basename)[0]
    
    # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)  # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¸Šä¸€çº§ç›®å½•ï¼‰
    output_dir = os.path.join(project_root, 'output_pngs')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, f'{log_name}.png')
    
    print(f"Analyzing log file: {log_file}")
    print(f"Output will be saved to: {output_path}\n")
    
    print("Parsing log file...")
    data = parse_log_file(log_file)
    
    print(f"\nFound {len(data)} metric types:")
    for metric_type, datasets in data.items():
        print(f"  {metric_type}: {list(datasets.keys())}")
        for dataset, values in datasets.items():
            print(f"    {dataset}: {len(values)} data points")
    
    # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„ step
    best_step, best_avg_score, best_details = find_best_step(data)
    if best_step is not None:
        print("\n" + "="*70)
        print("ğŸ† Best Validation Score")
        print("="*70)
        print(f"Step: {best_step}")
        print(f"Average test_score: {best_avg_score:.6f}")
        print("\nDetailed scores by dataset:")
        for dataset, score in sorted(best_details.items()):
            print(f"  {dataset:20s}: {score:.6f}")
        print("="*70)
    
    print("\nGenerating plot...")
    plot_combined_metrics(data, output_path)
    
    print("\nâœ“ Analysis complete!")

# python scripts/analyze_grpo_log.py A2.log