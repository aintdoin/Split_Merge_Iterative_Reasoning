""" 
åˆå¹¶hotpotæ•°æ®å¤„ç†å’ŒBest-of-Nè¿‡æ»¤çš„å®Œæ•´pipeline
1. å…ˆå¤„ç†answerableæ ·æœ¬ï¼ˆsize/2æ¡ï¼‰
2. ç”Ÿæˆunanswerableæ ·æœ¬å¹¶ä½¿ç”¨Best-of-32è¿‡æ»¤ï¼Œç›´åˆ°è·å¾—è¶³å¤Ÿçš„æ ·æœ¬ï¼ˆsize/2æ¡ï¼‰
3. æ··åˆå¹¶ä¿å­˜
"""

import os
import sys

# CRITICAL: Must set this before ANY imports that might use torch/CUDA
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

import pandas as pd
import json
from tqdm import tqdm
import numpy as np
import argparse
from pathlib import Path
from datasets import Dataset
import ast
import random

# Add parent directory to path to import verl modules
# File is in data_preprocess/, so we need to go up one level to reach project root
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Optional imports - requests for API
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("è­¦å‘Š: requestsä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨APIæ¨¡å¼")


def make_prefix_unified(dp, template_type):
    """ç»Ÿä¸€çš„promptå‰ç¼€ï¼Œç”¨äºanswerableå’Œunanswerableæ ·æœ¬"""
    question = dp.get('question', 'no question')
    documents_str = dp.get('documents', '[]')
    
    # è§£æå¹¶æ ¼å¼åŒ–documents
    try:
        documents_list = ast.literal_eval(documents_str)
    except:
        documents_list = []
    
    formatted_docs = []
    for doc in documents_list:
        if isinstance(doc, list) and len(doc) == 2:
            title, sentences = doc
            if isinstance(sentences, list):
                text = ' '.join(str(s) for s in sentences)
            else:
                text = str(sentences)
            formatted_docs.append(f"Document '{title}': {text}")
    
    documents_context = "\n".join(formatted_docs) if formatted_docs else "No references provided."
    
    user_content = f"""**References:**
{documents_context}

**Question:**
{question}"""
    
    system_prompt = """You are a helpful assistant. You are given a Question and References.

Your task: answer the Question only using factual information contained in the References. Do not use any external knowledge or your own knowledge.

**CRITICAL - You MUST follow this EXACT format:**
<think>
1. [First reasoning step]
2. [Second reasoning step]
3. [Third reasoning step]
...
</think>
<answer>Your final answer</answer>

**Rules (STRICTLY ENFORCED):**
1. Put reasoning in <think></think> tags
2. Use numbered steps (1., 2., 3., ...) in your <think> section for clear structured reasoning
3. NEVER start with anything other than <think> or <answer>
4. The <answer> tag MUST contain your final answer

Remember: Any response without proper <answer></answer> tags is INCORRECT."""
    
    if template_type in ['qwen']:
        prefix = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_content}<|im_end|>
<|im_start|>assistant
Let me solve this step by step.
<think>"""
    elif template_type in ['llama']:
        prefix = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Let me solve this step by step.
<think>"""
    else:
        prefix = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_content}<|im_end|>
<|im_start|>assistant
Let me solve this step by step.
<think>"""
    return prefix


def gen_from_jsonl(path):
    """ä»JSONLæ–‡ä»¶åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºdatasetæ ¼å¼"""
    with open(path) as f:
        for line in f:
            data = json.loads(line)    
            if 'supporting_facts' in data:
                evidence = []
                for fact in data['supporting_facts']:
                    title, sent_idx = fact
                    for doc in data['context']:
                        if doc[0] == title:
                            doc_text = " ".join(doc[1])
                            evidence.append(doc_text)
                            break
                data['evidences'] = str(evidence)
                data['supporting_facts'] = str(data['supporting_facts'])
            if 'context' in data:
                data['documents'] = str(data['context'])
                del data['context']


            if '_id' in data:
                extra_info = data.get('extra_info', {})
                if not isinstance(extra_info, dict):
                    extra_info = {}
                extra_info['sample_id'] = str(data['_id'])
                data['extra_info'] = extra_info
            yield data


def _has_valid_format(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æœ‰æœ‰æ•ˆçš„<answer></answer>æ ¼å¼"""
    try:
        a_s = text.count('<answer>')
        a_e = text.count('</answer>')
        if a_s != 1 or a_e != 1:
            return False
        ps = text.find('<answer>')
        pe = text.find('</answer>')
        if ps == -1 or pe == -1 or ps >= pe:
            return False
        content = text[ps + len('<answer>'):pe].strip()
        if len(content) == 0:
            return False
        return True
    except Exception:
        return False


def _is_idk_answer(text: str) -> bool:
    """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ˜¯IDK/ä¸ç¡®å®šçš„è¡¨è¾¾"""
    if not text:
        return False
    text_lower = text.strip().lower()
    idk_markers = [
        "i don't know", "i dont know", "i do not know",
        "i'm not sure", "i am not sure", "not sure",
        "cannot answer", "can't answer", "unable to answer",
        "cannot determine", "can't determine", "unable to determine",
        "insufficient information", "not enough information",
        "no sufficient information", "lack of information",
        "unknown", "unclear", "uncertain",
    ]
    return any(marker in text_lower for marker in idk_markers)


def call_api_for_candidates(prompt: str, api_base: str, model_name: str, api_key: str,
                            n: int, temperature: float, top_p: float, top_k: int, max_tokens: int) -> list:
    """
    è°ƒç”¨APIç”ŸæˆNä¸ªå€™é€‰å›ç­”
    è¿”å›ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
    """
    if not REQUESTS_AVAILABLE:
        raise RuntimeError("requestsåº“ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨APIæ¨¡å¼")
    
    base = api_base.rstrip('/')
    chat_url = base + '/v1/chat/completions'
    comp_url = base + '/v1/completions'
    
    headers = {'Content-Type': 'application/json'}
    if api_key:
        headers['Authorization'] = f'Bearer {api_key}'
    
    # å°è¯•chat endpoint
    chat_payload = {
        'model': model_name,
        'messages': [
            {'role': 'system', 'content': 'You are a helpful assistant.'},
            {'role': 'user', 'content': prompt}
        ],
        'temperature': temperature,
        'top_p': top_p,
        'max_tokens': max_tokens,
        'n': n,
        'stream': False,
    }
    
    resp = requests.post(chat_url, json=chat_payload, headers=headers, timeout=120)
    if resp.status_code == 200:
        data = resp.json()
        candidates = []
        try:
            for choice in data.get('choices', []):
                text = choice.get('message', {}).get('content', '')
                if text and text.strip():
                    candidates.append(text.strip())
        except Exception:
            pass
        if candidates:
            return candidates
    
    # å¤‡ç”¨ï¼šcompletions endpoint
    comp_payload = {
        'model': model_name,
        'prompt': prompt,
        'temperature': temperature,
        'top_p': top_p,
        'max_tokens': max_tokens,
        'n': n,
        'stream': False,
    }
    
    resp2 = requests.post(comp_url, json=comp_payload, headers=headers, timeout=120)
    if resp2.status_code != 200:
        print(f"\nâš ï¸  APIé”™è¯¯è¯¦æƒ…:")
        print(f"  çŠ¶æ€: {resp2.status_code}")
        print(f"  URL: {comp_url}")
        print(f"  æ¨¡å‹: {model_name}")
        print(f"  å“åº”: {resp2.text[:300]}")
        return []
    
    data2 = resp2.json()
    candidates = []
    for choice in data2.get('choices', []):
        text = choice.get('text', '').strip()
        if text:
            candidates.append(text)
    
    return candidates if candidates else []


def evaluate_sample_best_of_n(sample_dict, prompt, args, llm, sampling_params, postprocessor):
    """
    ä½¿ç”¨Best-of-Nç­–ç•¥è¯„ä¼°å•ä¸ªæ ·æœ¬
    è¿”å›: (is_truly_unanswerable: bool, best_reward: float)
    
    å¦‚æœ32æ¬¡æ¨ç†ä¸­æœ‰ä»»ä½•ä¸€æ¬¡æˆåŠŸå›ç­”ï¼ˆéIDKä¸”æ­£ç¡®ï¼‰ï¼Œåˆ™è¿”å›Falseï¼ˆä¸æ˜¯çœŸæ­£çš„unanswerableï¼‰
    åªæœ‰32æ¬¡å…¨éƒ¨å¤±è´¥ï¼Œæ‰è¿”å›Trueï¼ˆæ˜¯çœŸæ­£çš„unanswerableï¼‰
    """
    import re
    
    # æå–å…ƒæ•°æ®
    extra_info = sample_dict.get('extra_info', {})
    if isinstance(extra_info, str):
        try:
            extra_info = json.loads(extra_info)
        except:
            extra_info = {}
    
    question = sample_dict.get('question', '')
    ground_truth = sample_dict.get('answer', '')
    answer_aliases = extra_info.get('answer_aliases', [])
    if isinstance(answer_aliases, np.ndarray):
        answer_aliases = answer_aliases.tolist()
    elif answer_aliases is None:
        answer_aliases = []
    
    # ç”ŸæˆNä¸ªå€™é€‰å›ç­”
    if args.use_api:
        # APIæ¨¡å¼
        candidates = call_api_for_candidates(
            prompt, args.api_base, args.model_name, args.api_key,
            args.n_candidates, args.temperature, args.top_p, args.top_k, args.max_tokens
        )
        
        if not candidates:
            print(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return False, -2
        
        # è¯„ä¼°æ‰€æœ‰å€™é€‰
        has_correct_answer = False
        best_reward = -1
        
        for candidate_text in candidates:
            # æ£€æŸ¥æ ¼å¼
            if not _has_valid_format(candidate_text):
                reward = -1
            else:
                # æå–ç­”æ¡ˆ
                match = re.search(r'<answer>\s*(.*?)\s*</answer>', candidate_text, re.DOTALL | re.IGNORECASE)
                extracted_answer = match.group(1).strip() if match else candidate_text.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯IDK
                is_idk = _is_idk_answer(extracted_answer)
                
                # å°è¯•æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆ
                reward_scores = []
                all_answers = [ground_truth]
                if answer_aliases and len(answer_aliases) > 0:
                    all_answers.extend(answer_aliases)
                
                for ans in all_answers:
                    if ans:
                        try:
                            score = postprocessor.judge_answer_correctness(
                                predicted_answer=candidate_text,
                                ground_truth_answer=ans,
                                question=question,
                                answerable=False
                            )
                            reward_scores.append(score)
                        except Exception:
                            continue
                
                reward = max(reward_scores) if reward_scores else 0
                
                # åªæœ‰éIDKä¸”æ­£ç¡®æ‰ç®—æˆåŠŸå›ç­”
                if reward >= 0.999 and not is_idk:
                    has_correct_answer = True
            
            if reward > best_reward:
                best_reward = reward
        
        # è¿”å›ç»“æœï¼šå¦‚æœæœ‰æ­£ç¡®ç­”æ¡ˆï¼Œåˆ™ä¸æ˜¯çœŸæ­£çš„unanswerable
        return not has_correct_answer, best_reward
        
    else:
        # æœ¬åœ°vLLMæ¨¡å¼
        outputs = llm.generate([prompt], sampling_params)
        output = outputs[0]
        
        has_correct_answer = False
        best_reward = -1
        
        for candidate_output in output.outputs:
            generated_text = candidate_output.text
            
            # æ£€æŸ¥æ ¼å¼
            if not _has_valid_format(generated_text):
                reward = -1
            else:
                # æå–ç­”æ¡ˆ
                match = re.search(r'<answer>\s*(.*?)\s*</answer>', generated_text, re.DOTALL | re.IGNORECASE)
                extracted_answer = match.group(1).strip() if match else generated_text.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯IDK
                is_idk = _is_idk_answer(extracted_answer)
                
                # å°è¯•æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆ
                reward_scores = []
                all_answers = [ground_truth]
                if answer_aliases and len(answer_aliases) > 0:
                    all_answers.extend(answer_aliases)
                
                for ans in all_answers:
                    if ans:
                        try:
                            score = postprocessor.judge_answer_correctness(
                                predicted_answer=generated_text,
                                ground_truth_answer=ans,
                                question=question,
                                answerable=False
                            )
                            reward_scores.append(score)
                        except Exception:
                            continue
                
                reward = max(reward_scores) if reward_scores else 0
                
                # åªæœ‰éIDKä¸”æ­£ç¡®æ‰ç®—æˆåŠŸå›ç­”
                if reward >= 0.999 and not is_idk:
                    has_correct_answer = True
            
            if reward > best_reward:
                best_reward = reward
        
        return not has_correct_answer, best_reward


def create_answerable_samples(dataset, num_samples, template_type):
    """åˆ›å»ºanswerableæ ·æœ¬"""
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤1: åˆ›å»º{num_samples}æ¡ANSWERABLEæ ·æœ¬")
    print(f"{'='*80}")
    
    answerable_dataset = dataset.select(range(num_samples))
    print(f"é€‰å–äº†{len(answerable_dataset)}ä¸ªæ ·æœ¬ç”¨äºanswerable")
    
    answerable_samples = []
    for i in range(len(answerable_dataset)):
        sample = answerable_dataset[i]
        answerable_sample = {
            'question': sample.get('question', ''),
            'documents': sample.get('documents', '[]'),
            'answer': sample['answer'],
            'data_source': sample.get('data_source', 'hotpot'),
            'evidences': sample.get('evidences', '[]'),
            'extra_info': sample.get('extra_info', {}),
        }
        
        # è®¾ç½®answerableä¸ºTrue
        if isinstance(answerable_sample['extra_info'], dict):
            answerable_sample['extra_info']['answerable'] = True
        else:
            answerable_sample['extra_info'] = {'answerable': True}
        
        answerable_samples.append(answerable_sample)
    
    print(f"âœ“ åˆ›å»ºäº†{len(answerable_samples)}æ¡answerableæ ·æœ¬")
    return answerable_samples


def create_unanswerable_samples_with_filter(dataset, start_idx, num_samples, template_type, 
                                           args, llm, sampling_params, postprocessor):
    """
    åˆ›å»ºunanswerableæ ·æœ¬å¹¶å®æ—¶è¿‡æ»¤
    è¾¹ç”Ÿæˆè¾¹è¿‡æ»¤ï¼Œç›´åˆ°è·å¾—è¶³å¤Ÿçš„æ ·æœ¬
    """
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤2: åˆ›å»ºå¹¶è¿‡æ»¤{num_samples}æ¡UNANSWERABLEæ ·æœ¬")
    print(f"{'='*80}")
    print(f"ä½¿ç”¨Best-of-{args.n_candidates}ç­–ç•¥")
    print(f"åªä¿ç•™32æ¬¡æ¨ç†å…¨éƒ¨å¤±è´¥çš„æ ·æœ¬\n")
    
    kept_samples = []
    removed_count = 0
    processed_count = 0
    
    # æˆ‘ä»¬éœ€è¦ä¸¤å€çš„æ ·æœ¬ï¼šä¸€éƒ¨åˆ†ä½œä¸ºé—®é¢˜ï¼Œå¦ä¸€éƒ¨åˆ†ä½œä¸ºä¸åŒ¹é…çš„æ–‡æ¡£
    max_samples_to_try = min(len(dataset) - start_idx, num_samples * 10)  # æœ€å¤šå°è¯•10å€æ•°é‡
    
    pbar = tqdm(total=num_samples, desc="è¿‡æ»¤unanswerableæ ·æœ¬")
    
    idx = start_idx
    while len(kept_samples) < num_samples and idx < len(dataset) - 1:
        question_sample = dataset[idx]
        
        # è·å–é—®é¢˜å’ŒåŸå§‹æ–‡æ¡£
        question = question_sample.get('question', '')
        answer = question_sample['answer']
        
        # è§£æåŸå§‹æ–‡æ¡£
        try:
            original_documents = ast.literal_eval(question_sample.get('documents', '[]'))
        except:
            original_documents = []
        
        # è§£æsupporting_factsä»¥è¯†åˆ«å…³é”®æ–‡æ¡£
        supporting_facts = question_sample.get('supporting_facts', None)
        if supporting_facts is None:
            extra_info = question_sample.get('extra_info', {})
            if isinstance(extra_info, str):
                try:
                    extra_info = ast.literal_eval(extra_info)
                except:
                    extra_info = {}
            supporting_facts = extra_info.get('supporting_facts', [])
        
        if isinstance(supporting_facts, str):
            try:
                supporting_facts = ast.literal_eval(supporting_facts)
            except:
                supporting_facts = []
        
        # æå–å”¯ä¸€çš„æ–‡æ¡£æ ‡é¢˜
        supporting_doc_titles = []
        if isinstance(supporting_facts, list):
            for fact in supporting_facts:
                if isinstance(fact, list) and len(fact) >= 1:
                    title = fact[0]
                    if title not in supporting_doc_titles:
                        supporting_doc_titles.append(title)
        
        # ç­–ç•¥ï¼šç§»é™¤å…³é”®æ”¯æ’‘æ–‡æ¡£ï¼ˆé™¤äº†èµ·å§‹èŠ‚ç‚¹ï¼‰
        if supporting_doc_titles and len(supporting_doc_titles) > 1 and original_documents:
            starting_node = supporting_doc_titles[0]
            removal_candidates = supporting_doc_titles[1:]
            
            if len(supporting_doc_titles) >= 4:
                num_to_remove = min(2, len(removal_candidates))
            else:
                num_to_remove = 1
            
            docs_to_remove = random.sample(removal_candidates, num_to_remove)
            
            # ç§»é™¤é€‰å®šçš„æ–‡æ¡£
            modified_documents = original_documents.copy()
            for doc_title in docs_to_remove:
                modified_documents = [doc for doc in modified_documents 
                                    if not (isinstance(doc, list) and len(doc) >= 2 and doc[0] == doc_title)]
        else:
            modified_documents = original_documents
        
        # å¢å¼ºevidences
        evidences = question_sample.get('evidences', '[]')
        try:
            evidences_list = ast.literal_eval(evidences) if isinstance(evidences, str) else evidences
        except:
            evidences_list = []
        
        augmented_evidences = evidences_list
        
        # åˆ›å»ºunanswerableæ ·æœ¬
        unanswerable_sample = {
            'question': question,
            'documents': str(modified_documents),
            'answer': answer,
            'data_source': question_sample.get('data_source', 'hotpot'),
            'evidences': str(augmented_evidences),
            'extra_info': question_sample.get('extra_info', {}),
        }
        
        # æ›´æ–°answerableä¸ºFalseï¼ˆä¸ä¿®æ”¹æˆ–åˆ›å»º sample_idï¼‰
        if isinstance(unanswerable_sample['extra_info'], dict):
            unanswerable_sample['extra_info']['answerable'] = False
        else:
            unanswerable_sample['extra_info'] = {'answerable': False}
        
        # ç”Ÿæˆpromptç”¨äºè¿‡æ»¤
        prompt = make_prefix_unified(unanswerable_sample, template_type)
        
        # ä½¿ç”¨Best-of-Nè¯„ä¼°
        is_truly_unanswerable, best_reward = evaluate_sample_best_of_n(
            unanswerable_sample, prompt, args, llm, sampling_params, postprocessor
        )
        
        processed_count += 1
        
        if is_truly_unanswerable:
            # é€šè¿‡è¿‡æ»¤ï¼š32æ¬¡å…¨éƒ¨å¤±è´¥
            kept_samples.append(unanswerable_sample)
            pbar.update(1)
            pbar.set_postfix({
                'ä¿ç•™': len(kept_samples),
                'ç§»é™¤': removed_count,
                'å¤„ç†': processed_count,
                'ç§»é™¤ç‡': f'{removed_count/processed_count*100:.1f}%'
            })
        else:
            # æœªé€šè¿‡è¿‡æ»¤ï¼šè‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸå›ç­”
            removed_count += 1
        
        idx += 1
        
        if idx >= len(dataset) - 1:
            print(f"\nâš ï¸ è­¦å‘Š: å·²éå†å®Œæ‰€æœ‰å¯ç”¨æ ·æœ¬")
            print(f"   åªè·å¾—äº†{len(kept_samples)}/{num_samples}æ¡åˆæ ¼çš„unanswerableæ ·æœ¬")
            break
    
    pbar.close()
    
    print(f"\n{'='*80}")
    print("è¿‡æ»¤ç»“æœ")
    print(f"{'='*80}")
    print(f"å¤„ç†çš„æ ·æœ¬æ€»æ•°: {processed_count}")
    print(f"ä¿ç•™çš„æ ·æœ¬(çœŸæ­£unanswerable): {len(kept_samples)}")
    print(f"ç§»é™¤çš„æ ·æœ¬(å¯è¢«å›ç­”): {removed_count}")
    print(f"ç§»é™¤ç‡: {removed_count/processed_count*100:.1f}%")
    
    return kept_samples


def main():
    parser = argparse.ArgumentParser(description='åˆå¹¶hotpotæ•°æ®å¤„ç†å’ŒBest-of-Nè¿‡æ»¤')
    parser.add_argument('--type', type=str, default='train', help='trainæˆ–test')
    parser.add_argument('--template_type', type=str, default='deepseek-r1-distill-qwen')
    parser.add_argument('--size', type=int, required=True, help='ç›®æ ‡æ ·æœ¬æ€»æ•°ï¼ˆå°†å¹³åˆ†ä¸ºanswerableå’Œunanswerableï¼‰')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--data-path', type=str, default=None, help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    
    # æ¨¡å‹/APIé…ç½®
    parser.add_argument('--model-path', type=str, default='', help='æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆvLLMæ¨¡å¼ï¼‰')
    parser.add_argument('--use-api', action='store_true', help='ä½¿ç”¨APIæ¨¡å¼è€Œéæœ¬åœ°vLLM')
    parser.add_argument('--api-base', type=str, default='http://localhost:8000', help='APIåŸºç¡€URL')
    parser.add_argument('--api-key', type=str, default='', help='APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--model-name', type=str, default='', help='APIæ¨¡å‹åç§°')
    
    # è¿‡æ»¤å‚æ•°
    parser.add_argument('--n-candidates', type=int, default=32, help='æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„å€™é€‰å›ç­”æ•°é‡')
    parser.add_argument('--temperature', type=float, default=1.0, help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--top-p', type=float, default=0.95, help='Top-pé‡‡æ ·å‚æ•°')
    parser.add_argument('--top-k', type=int, default=100, help='Top-ké‡‡æ ·å‚æ•°')
    parser.add_argument('--max-tokens', type=int, default=2048, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    
    # vLLMå‚æ•°
    parser.add_argument('--max-model-len', type=int, default=24500, help='vLLMæœ€å¤§æ¨¡å‹é•¿åº¦')
    parser.add_argument('--tensor-parallel-size', type=int, default=1, help='vLLMå¼ é‡å¹¶è¡Œå¤§å°')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("HOTPOTæ•°æ®å¤„ç† + BEST-OF-Nè¿‡æ»¤ åˆå¹¶PIPELINE")
    print("="*80)
    print(f"æ€»ç›®æ ‡æ ·æœ¬æ•°: {args.size}")
    print(f"  - Answerable: {args.size // 2}")
    print(f"  - Unanswerable (éœ€è¿‡æ»¤): {args.size // 2}")
    print(f"æ¨¡å¼: {'API' if args.use_api else 'æœ¬åœ°vLLM'}")
    if args.use_api:
        print(f"API Base: {args.api_base}")
        print(f"æ¨¡å‹: {args.model_name}")
    else:
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"Best-of-N: {args.n_candidates}")
    print("="*80)
    
    # ç¡®å®šæ•°æ®è·¯å¾„
    if args.data_path:
        data_path = args.data_path
    elif args.type == 'train':
        data_path = '/mnt/shared-storage-user/liyafu/runquan/hotpot/hotpot_train_v1.1.jsonl'
    else:
        data_path = '/mnt/shared-storage-user/liyafu/runquan/hotpot/hotpot_dev_distractor_v1.jsonl'
    
    # è®¡ç®—æ¯ç±»æ ·æœ¬æ•°é‡
    answerable_size = args.size // 2
    unanswerable_size = args.size // 2
    
    # åŠ è½½åŸå§‹æ•°æ®é›†
    print(f"\nğŸ“‚ ä»{data_path}åŠ è½½æ•°æ®...")
    raw_dataset = Dataset.from_generator(gen_from_jsonl, gen_kwargs={'path': data_path})
    print(f"   âœ“ åŸå§‹æ•°æ®é›†é•¿åº¦: {len(raw_dataset)}")
    
    # æ‰“ä¹±å¹¶é€‰æ‹©è¶³å¤Ÿçš„æ ·æœ¬
    total_needed = answerable_size + unanswerable_size * 10  # ä¸ºunanswerableé¢„ç•™æ›´å¤šæ ·æœ¬
    dataset = raw_dataset.shuffle(seed=42).select(range(min(total_needed, len(raw_dataset))))
    print(f"   âœ“ é€‰æ‹©äº†{len(dataset)}ä¸ªæ ·æœ¬ç”¨äºå¤„ç†")
    
    # åˆå§‹åŒ–æ¨¡å‹/API
    llm = None
    sampling_params = None
    
    if args.use_api:
        print(f"\nğŸŒ æµ‹è¯•APIè¿æ¥...")
        if not REQUESTS_AVAILABLE:
            print("é”™è¯¯: requestsåº“ä¸å¯ç”¨!")
            return
        if not args.model_name:
            print("é”™è¯¯: APIæ¨¡å¼éœ€è¦--model-nameå‚æ•°!")
            return
        
        base = args.api_base.rstrip('/')
        models_url = base + '/v1/models'
        try:
            resp = requests.get(models_url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                available_models = [m.get('id', 'unknown') for m in data.get('data', [])]
                print(f"  âœ“ APIå¯è®¿é—®")
                print(f"  å¯ç”¨æ¨¡å‹: {available_models}")
                if args.model_name not in available_models and available_models:
                    print(f"  âš ï¸ è­¦å‘Š: '{args.model_name}'ä¸åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­")
            else:
                print(f"  âš ï¸ è­¦å‘Š: æ— æ³•è®¿é—®modelsç«¯ç‚¹ (çŠ¶æ€: {resp.status_code})")
        except Exception as e:
            print(f"  âš ï¸ è­¦å‘Š: æ— æ³•è¿æ¥åˆ°API: {e}")
            return
    else:
        print(f"\nğŸ”§ ä»{args.model_path}åŠ è½½æ¨¡å‹...")
        if not args.model_path:
            print("é”™è¯¯: æœ¬åœ°æ¨¡å¼éœ€è¦--model-pathå‚æ•°!")
            return
        
        # å»¶è¿Ÿå¯¼å…¥vLLMï¼Œåªåœ¨å®é™…ä½¿ç”¨æ—¶å¯¼å…¥
        try:
            from vllm import LLM, SamplingParams
            print("   âœ“ vLLMæ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError:
            print("é”™è¯¯: vLLMä¸å¯ç”¨! è¯·ä½¿ç”¨--use-apiåˆ‡æ¢åˆ°APIæ¨¡å¼ã€‚")
            return
        
        llm = LLM(
            model=args.model_path,
            trust_remote_code=True,
            dtype="bfloat16",
            tensor_parallel_size=args.tensor_parallel_size,
            max_model_len=args.max_model_len
        )
        
        sampling_params = SamplingParams(
            temperature=args.temperature,
            top_p=args.top_p,
            top_k=args.top_k,
            max_tokens=args.max_tokens,
            n=args.n_candidates,
        )
        print("   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åˆå§‹åŒ–answer postprocessor (åœ¨vLLMåŠ è½½ä¹‹åå¯¼å…¥ï¼Œé¿å…CUDAå†²çª)
    print("\nğŸ” åˆå§‹åŒ–answer postprocessor...")
    from verl.utils.reward_score.answer_postprocessor import get_postprocessor
    postprocessor = get_postprocessor()
    print("   âœ“ Postprocessoråˆå§‹åŒ–å®Œæˆ")
    
    # æ­¥éª¤1: åˆ›å»ºanswerableæ ·æœ¬
    answerable_samples = create_answerable_samples(dataset, answerable_size, args.template_type)
    
    # æ­¥éª¤2: åˆ›å»ºå¹¶è¿‡æ»¤unanswerableæ ·æœ¬
    unanswerable_samples = create_unanswerable_samples_with_filter(
        dataset, answerable_size, unanswerable_size, args.template_type,
        args, llm, sampling_params, postprocessor
    )
    
    # åˆå¹¶æ ·æœ¬
    print(f"\n{'='*80}")
    print("æ­¥éª¤3: åˆå¹¶å¹¶ä¿å­˜æ•°æ®é›†")
    print(f"{'='*80}")
    
    all_samples = answerable_samples + unanswerable_samples
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"  - Answerable: {len(answerable_samples)}")
    print(f"  - Unanswerable: {len(unanswerable_samples)}")
    
    # è½¬æ¢ä¸ºDataset
    combined_dataset = Dataset.from_list(all_samples)
    
    # é‡æ–°ç”Ÿæˆprompt
    def regenerate_prompt(example, idx):
        question = make_prefix_unified(example, template_type=args.template_type)
        return {
            "prompt": question,
            "question": example['question'],
            "answer": example['answer'],
            "data_source": example['data_source'],
            "extra_info": example['extra_info'],
            "documents": example['documents'],
            "evidences": example['evidences'],
        }
    
    print("\nç”Ÿæˆprompt...")
    combined_dataset = combined_dataset.map(regenerate_prompt, with_indices=True)
    
    # æ´—æ··æ•°æ®
    print("æ´—æ··æ•°æ®é›†...")
    combined_dataset = combined_dataset.shuffle(seed=42)
    
    # ä¿å­˜
    output_dir = f'data/hotpot/{args.template_type}'
    os.makedirs(os.path.expanduser(output_dir), exist_ok=True)
    
    if args.type == 'train':
        output_file = os.path.join(output_dir, 'train.parquet')
    else:
        output_file = os.path.join(output_dir, 'test.parquet')
    
    combined_dataset.to_parquet(output_file)
    print(f"\nğŸ’¾ ä¿å­˜åˆ°{output_file}")
    print(f"   âœ“ æœ€ç»ˆæ•°æ®é›†: {len(combined_dataset)}ä¸ªæ ·æœ¬")
    
    # éªŒè¯
    df_verify = pd.read_parquet(output_file)
    n_false = sum(1 for _, row in df_verify.iterrows() 
                  if isinstance(row.get('extra_info'), (dict, str)) and 
                  (json.loads(row['extra_info']) if isinstance(row['extra_info'], str) else row['extra_info']).get('answerable') == False)
    n_true = sum(1 for _, row in df_verify.iterrows() 
                 if isinstance(row.get('extra_info'), (dict, str)) and 
                 (json.loads(row['extra_info']) if isinstance(row['extra_info'], str) else row['extra_info']).get('answerable') == True)
    
    print(f"\néªŒè¯:")
    print(f"   âœ“ answerable=True: {n_true}")
    print(f"   âœ“ answerable=False: {n_false}")
    
    # æ¸…ç†
    if llm is not None:
        try:
            llm.shutdown()
        except Exception:
            pass
    
    print("\nâœ… å®Œæˆ!\n")


if __name__ == '__main__':
    main()

